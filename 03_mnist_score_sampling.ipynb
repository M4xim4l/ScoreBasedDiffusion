{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import score_based_sampling.models as models\n",
    "from score_based_sampling.train_score_model import train_score_model, estimate_max_distance_in_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "device = torch.device(f'cuda:{gpu_id}')\n",
    "\n",
    "#Load data\n",
    "train_bs = 128\n",
    "val_bs = 512\n",
    "\n",
    "\n",
    "dataset_dir = 'datasets/MNIST'\n",
    "p = pathlib.Path(dataset_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(dataset_dir, train=True, download=True,\n",
    "                           transform=torchvision.transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = torchvision.datasets.MNIST(dataset_dir, train=False, download=True,\n",
    "                           transform=torchvision.transforms.ToTensor())\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_bs, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sigma 15.982 - Min Sigma 0.010 - Num scales 250 - Gamma 0.971\n"
     ]
    }
   ],
   "source": [
    "#Noise scales\n",
    "N_sigma_estimation_samples = min(10_000, len(train_dataset))\n",
    "sigma_estimation_samples = torch.zeros((N_sigma_estimation_samples, 1, 28, 28))\n",
    "collected_samples = 0\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "while collected_samples < N_sigma_estimation_samples:\n",
    "    data = next(train_iter)[0]\n",
    "    samples_to_collect_from_batch = min(len(data), N_sigma_estimation_samples - collected_samples)\n",
    "    data = data[:samples_to_collect_from_batch]\n",
    "    sigma_estimation_samples[collected_samples:(collected_samples+samples_to_collect_from_batch)] = data\n",
    "\n",
    "    collected_samples += samples_to_collect_from_batch\n",
    "\n",
    "\n",
    "max_sigma = estimate_max_distance_in_dataset(sigma_estimation_samples.view(N_sigma_estimation_samples, -1))\n",
    "min_sigma_target = 0.01\n",
    "N_noise_scales = 250\n",
    "\n",
    "#geometric series with common ratio gamma\n",
    "gamma = (min_sigma_target / max_sigma) ** (1 / (N_noise_scales - 1))\n",
    "sigmas = max_sigma * gamma ** (torch.arange(0, N_noise_scales))\n",
    "sigmas = sigmas.to(device)\n",
    "min_sigma = sigmas[-1]\n",
    "\n",
    "print(f'Max sigma {max_sigma:.3f} - Min Sigma {min_sigma:.3f} - Num scales {N_noise_scales} - Gamma {gamma:.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting score net training...\n",
      "Epoch 0 - Avg train loss 403.3982849121094\n",
      "Val loss 391.946533203125\n",
      "Epoch 1 - Avg train loss 387.794921875\n",
      "Val loss 368.784423828125\n",
      "Epoch 2 - Avg train loss 308.7484130859375\n",
      "Val loss 279.36328125\n",
      "Epoch 3 - Avg train loss 268.6764221191406\n",
      "Val loss 289.85467529296875\n",
      "Epoch 4 - Avg train loss 253.82504272460938\n",
      "Val loss 245.0906982421875\n",
      "Epoch 5 - Avg train loss 243.81289672851562\n",
      "Val loss 241.29115295410156\n",
      "Epoch 6 - Avg train loss 241.0709686279297\n",
      "Val loss 230.50640869140625\n",
      "Epoch 7 - Avg train loss 229.1835479736328\n",
      "Val loss 228.08120727539062\n",
      "Epoch 8 - Avg train loss 227.80726623535156\n",
      "Val loss 219.05279541015625\n",
      "Epoch 9 - Avg train loss 220.03269958496094\n",
      "Val loss 228.14109802246094\n",
      "Epoch 10 - Avg train loss 226.57553100585938\n",
      "Val loss 211.39450073242188\n",
      "Epoch 11 - Avg train loss 219.55264282226562\n",
      "Val loss 220.1234130859375\n",
      "Epoch 12 - Avg train loss 219.23519897460938\n",
      "Val loss 215.0110626220703\n",
      "Epoch 13 - Avg train loss 216.1722412109375\n",
      "Val loss 209.4766387939453\n",
      "Epoch 14 - Avg train loss 213.02452087402344\n",
      "Val loss 210.76248168945312\n",
      "Epoch 15 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 16 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 17 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 18 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 19 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 20 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 21 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 22 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 23 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 24 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 25 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 26 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 27 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 28 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 29 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 30 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 31 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 32 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 33 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 34 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 35 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 36 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 37 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 38 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 39 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 40 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 41 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 42 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 43 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 44 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 45 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 46 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 47 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 48 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 49 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 50 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 51 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 52 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 53 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 54 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 55 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 56 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 57 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 58 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 59 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 60 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 61 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 62 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 63 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 64 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 65 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 66 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 67 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 68 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 69 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 70 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 71 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 72 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 73 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 74 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 75 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 76 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 77 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 78 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 79 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 80 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 81 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 82 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 83 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 84 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 85 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 86 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 87 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 88 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 89 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 90 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 91 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 92 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 93 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 94 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 95 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 96 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 97 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 98 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 99 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 100 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 101 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 102 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 103 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 104 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 105 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 106 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 107 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 108 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 109 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 110 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 111 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 112 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 113 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 114 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 115 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 116 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 117 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 118 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 119 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 120 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 121 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 122 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 123 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 124 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 125 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 126 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 127 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 128 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 129 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 130 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 131 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 132 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 133 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 134 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 135 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 136 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 137 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 138 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 139 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 140 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 141 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 142 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 143 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 144 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 145 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 146 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 147 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 148 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 149 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 150 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 151 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 152 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 153 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 154 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 155 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 156 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 157 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 158 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 159 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 160 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 161 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 162 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 163 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 164 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 165 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 166 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 167 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 168 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 169 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 170 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 171 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 172 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 173 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 174 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 175 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 176 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 177 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 178 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 179 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 180 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 181 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 182 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 183 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 184 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 185 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 186 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 187 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 188 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 189 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 190 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 191 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 192 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 193 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 194 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 195 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 196 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 197 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 198 - Avg train loss nan\n",
      "Val loss nan\n",
      "Epoch 199 - Avg train loss nan\n",
      "Val loss nan\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "#load our model\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "optim = 'adam'\n",
    "\n",
    "modelname = 'ncsnv2'\n",
    "\n",
    "if modelname == 'unet':\n",
    "    model = models.get_MNIST_UNet(sigmas)\n",
    "elif modelname == 'ncsnv2':\n",
    "    model = models.get_MNIST_NCSNv2(sigmas)\n",
    "else:\n",
    "    raise NotImplemented\n",
    "\n",
    "model_chkpt_filename = f'mnist_{modelname}_{N_noise_scales}.pth'\n",
    "if os.path.isfile(model_chkpt_filename):\n",
    "    print('Loading model checkpoint from file')\n",
    "    model.load_state_dict(torch.load(model_chkpt_filename))\n",
    "    model.to(device)\n",
    "else:\n",
    "    model.to(device)\n",
    "    print('Starting score net training...')\n",
    "    train_score_model(model, sigmas, train_loader, lr, epochs, device, optim=optim, val_loader=val_loader)\n",
    "    torch.save(model.state_dict(), model_chkpt_filename)\n",
    "    print('Training done')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eq 4 0: 0.9424909949302673 - Gamma sqr 0.9424711465835571\n",
      "Epsilon Langevin 1.000000013351432e-10 - Achieved EQ4 value 0.9424909949302673\n"
     ]
    }
   ],
   "source": [
    "#Langevin samples - Multiple Noise Scales\n",
    "#Stepsize selection using Technique 4\n",
    "\n",
    "def calc_eq4(eps):\n",
    "    first_factor = (1 - eps / min_sigma**2)**(2*N_noise_scales)\n",
    "    inner_denom = min_sigma**2 - (min_sigma**2) * (1 - eps / min_sigma**2)**2\n",
    "    sec_factor = gamma**2 - (2*eps) / inner_denom\n",
    "    third_term = (2*eps) / inner_denom\n",
    "    return first_factor * sec_factor + third_term\n",
    "\n",
    "print(f'Eq 4 0: {calc_eq4(1e-10)} - Gamma sqr {gamma**2}')\n",
    "\n",
    "eps_search = torch.linspace(1e-10, 1e3, 1_000_000, device=device)\n",
    "eps_eq4_vals = calc_eq4(eps_search)\n",
    "best_eps_idx = torch.argmin(torch.abs(1 -  eps_eq4_vals))\n",
    "eps = eps_search[best_eps_idx]\n",
    "eq4_val_best_eps = eps_eq4_vals[best_eps_idx]\n",
    "\n",
    "print(f'Epsilon Langevin {eps} - Achieved EQ4 value {eq4_val_best_eps}')\n",
    "\n",
    "eps = 1e-4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:24, 10.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#https://abdulfatir.com/blog/2020/Langevin-Monte-Carlo/\n",
    "#https://en.wikipedia.org/wiki/Stochastic_gradient_Langevin_dynamics\n",
    "num_samples = 25\n",
    "sampling_steps_per_noise_scale = 5\n",
    "ns_samples = torch.rand((num_samples, 1, 28, 28), device=device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sigma_idx, sigma in tqdm(enumerate(sigmas)):\n",
    "        noise_scale_idcs = sigma_idx * torch.ones(num_samples, dtype=torch.long, device=device)\n",
    "        alpha = eps * (sigma / min_sigma)**2\n",
    "        for _ in range(sampling_steps_per_noise_scale):\n",
    "            score = model(ns_samples, noise_scale_idcs)\n",
    "            noise = torch.randn_like(ns_samples, device=device)\n",
    "            #print(f'score norm = {torch.sum(((alpha * score)**2).view(num_samples,-1), dim=1).mean():.3f}')\n",
    "            #print(f'noise norm = {torch.sum(((math.sqrt(2 * alpha) * noise)**2).view(num_samples,-1), dim=1).mean():.3f}')\n",
    "            #print(f'alpha = {alpha:.3f} - noise alpha {math.sqrt(2 * alpha)}')\n",
    "            ns_samples = ns_samples + alpha * score + alpha * noise #math.sqrt(2 * alpha) * noise\n",
    "\n",
    "ns_samples = ns_samples.detach().cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 25 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_cols = int(math.ceil(math.sqrt(num_samples)))\n",
    "N_rows = int(math.ceil(num_samples / N_cols))\n",
    "\n",
    "fig, axs = plt.subplots(N_rows, N_cols)\n",
    "for i in range(N_rows):\n",
    "    for j in range(N_cols):\n",
    "        lin_Idx = i * N_cols + j\n",
    "        img = ns_samples[lin_Idx]\n",
    "        img = torch.cat([img,img,img])\n",
    "        img = torchvision.transforms.functional.to_pil_image(img)\n",
    "        axs[i, j].imshow(np.asarray(img))\n",
    "        axs[i, j].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
